{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is part 3 of 3 jupyter notebooks that are made to build an AI tool that can help self-driving cars see."
      ],
      "metadata": {
        "id": "avgsblPUnuGC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd7Eem4jQWNb"
      },
      "source": [
        "We will see one of the most widely used state-of-the-art object detection algorithms -- YOLO. We'll apply this algorithm on both images and videos.\n",
        "\n",
        "Click on the image below to see some amazing results!\n",
        "\n",
        "[<img src=\"https://pjreddie.com/media/image/yologo_2.png\" width=\"400\"/>](https://www.youtube.com/watch?v=MPU2HistivI)\n",
        "\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WMqegZ_nVhd"
      },
      "source": [
        "In this notebook we'll be:\n",
        "1.   Understanding YOLO\n",
        "2.   Performing Bounding Box Prediction\n",
        "3.   Performing Object Detection on videos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoQdwHTR3nEg"
      },
      "source": [
        "#@title Run this to download data and prepare our environment! { display-mode: \"form\" }\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "import gdown\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D\n",
        "from keras.layers.merge import add, concatenate\n",
        "from keras.models import Model\n",
        "import struct\n",
        "import cv2\n",
        "from copy import deepcopy\n",
        "\n",
        "# Prepare data\n",
        "DATA_ROOT = '/content/data'\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "\n",
        "# image_url = 'https://drive.google.com/uc?id=12ZpZ5H0kJIkWk6y4ktGfqR5OTKofL7qw'\n",
        "# image_path = os.path.join(DATA_ROOT, 'image.jpg')\n",
        "# gdown.download(image_url, image_path, True)\n",
        "!wget -O /content/data/image.jpg \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20%20Object%20Detection%20(Autonomous%20Vehicles)/image.jpg\"\n",
        "\n",
        "# image2_url = 'https://drive.google.com/uc?id=1_WpFbGEuS2r19UeP6wekbcF0kb-0nH18'\n",
        "# image2_path = os.path.join(DATA_ROOT, 'image2.jpg')\n",
        "# gdown.download(image2_url, image2_path, True)\n",
        "!wget -O /content/data/image2.jpg \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20%20Object%20Detection%20(Autonomous%20Vehicles)/image2.jpg\"\n",
        "\n",
        "# video_url = 'https://drive.google.com/uc?id=1xFGjpzhZVYtNor9hJevvxysGESZJIMDz'\n",
        "# video_path = os.path.join(DATA_ROOT, 'video1.mp4')\n",
        "# gdown.download(video_url, video_path, True)\n",
        "!wget -O /content/data/video1.mp4 \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20%20Object%20Detection%20(Autonomous%20Vehicles)/6.mp4\"\n",
        "\n",
        "# model_url = 'https://drive.google.com/uc?id=19XKJWMKDfDlag2MR8ofjwvxhtr9BxqqN'\n",
        "model_path = os.path.join(DATA_ROOT, 'yolo_weights.h5')\n",
        "# gdown.download(model_url, model_path, True)\n",
        "!wget -O /content/data/yolo_weights.h5 \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20%20Object%20Detection%20(Autonomous%20Vehicles)/yolo.h5\"\n",
        "\n",
        "labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \\\n",
        "              \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \\\n",
        "              \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \\\n",
        "              \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \\\n",
        "              \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \\\n",
        "              \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \\\n",
        "              \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \\\n",
        "              \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \\\n",
        "              \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \\\n",
        "              \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n",
        "\n",
        "class BoundBox:\n",
        "    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n",
        "        self.xmin = xmin\n",
        "        self.ymin = ymin\n",
        "        self.xmax = xmax\n",
        "        self.ymax = ymax\n",
        "\n",
        "        self.objness = objness\n",
        "        self.classes = classes\n",
        "\n",
        "        self.label = -1\n",
        "        self.score = -1\n",
        "\n",
        "    def get_label(self):\n",
        "        if self.label == -1:\n",
        "            self.label = np.argmax(self.classes)\n",
        "\n",
        "        return self.label\n",
        "\n",
        "    def get_score(self):\n",
        "        if self.score == -1:\n",
        "            self.score = self.classes[self.get_label()]\n",
        "\n",
        "        return self.score\n",
        "\n",
        "def _interval_overlap(interval_a, interval_b):\n",
        "    x1, x2 = interval_a\n",
        "    x3, x4 = interval_b\n",
        "\n",
        "    if x3 < x1:\n",
        "        if x4 < x1:\n",
        "            return 0\n",
        "        else:\n",
        "            return min(x2,x4) - x1\n",
        "    else:\n",
        "        if x2 < x3:\n",
        "             return 0\n",
        "        else:\n",
        "            return min(x2,x4) - x3\n",
        "\n",
        "def _sigmoid(x):\n",
        "    return 1. / (1. + np.exp(-x))\n",
        "\n",
        "def bbox_iou(box1, box2):\n",
        "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
        "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n",
        "\n",
        "    intersect = intersect_w * intersect_h\n",
        "\n",
        "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
        "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
        "\n",
        "    union = w1*h1 + w2*h2 - intersect\n",
        "\n",
        "    return float(intersect) / union\n",
        "\n",
        "def preprocess_input(image_pil, net_h, net_w):\n",
        "    image = np.asarray(image_pil)\n",
        "    new_h, new_w, _ = image.shape\n",
        "\n",
        "    # determine the new size of the image\n",
        "    if (float(net_w)/new_w) < (float(net_h)/new_h):\n",
        "        new_h = (new_h * net_w)/new_w\n",
        "        new_w = net_w\n",
        "    else:\n",
        "        new_w = (new_w * net_h)/new_h\n",
        "        new_h = net_h\n",
        "\n",
        "    # resize the image to the new size\n",
        "    #resized = cv2.resize(image[:,:,::-1]/255., (int(new_w), int(new_h)))\n",
        "    resized = cv2.resize(image/255., (int(new_w), int(new_h)))\n",
        "\n",
        "    # embed the image into the standard letter box\n",
        "    new_image = np.ones((net_h, net_w, 3)) * 0.5\n",
        "    new_image[int((net_h-new_h)//2):int((net_h+new_h)//2), int((net_w-new_w)//2):int((net_w+new_w)//2), :] = resized\n",
        "    new_image = np.expand_dims(new_image, 0)\n",
        "\n",
        "    return new_image\n",
        "\n",
        "\n",
        "def decode_netout(netout_, obj_thresh, anchors_, image_h, image_w, net_h, net_w):\n",
        "    netout_all = deepcopy(netout_)\n",
        "    boxes_all = []\n",
        "    for i in range(len(netout_all)):\n",
        "      netout = netout_all[i][0]\n",
        "      anchors = anchors_[i]\n",
        "\n",
        "      grid_h, grid_w = netout.shape[:2]\n",
        "      nb_box = 3\n",
        "      netout = netout.reshape((grid_h, grid_w, nb_box, -1))\n",
        "      nb_class = netout.shape[-1] - 5\n",
        "\n",
        "      boxes = []\n",
        "\n",
        "      netout[..., :2]  = _sigmoid(netout[..., :2])\n",
        "      netout[..., 4:]  = _sigmoid(netout[..., 4:])\n",
        "      netout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n",
        "      netout[..., 5:] *= netout[..., 5:] > obj_thresh\n",
        "\n",
        "      for i in range(grid_h*grid_w):\n",
        "          row = i // grid_w\n",
        "          col = i % grid_w\n",
        "\n",
        "          for b in range(nb_box):\n",
        "              # 4th element is objectness score\n",
        "              objectness = netout[row][col][b][4]\n",
        "              #objectness = netout[..., :4]\n",
        "              # last elements are class probabilities\n",
        "              classes = netout[row][col][b][5:]\n",
        "\n",
        "              if((classes <= obj_thresh).all()): continue\n",
        "\n",
        "              # first 4 elements are x, y, w, and h\n",
        "              x, y, w, h = netout[row][col][b][:4]\n",
        "\n",
        "              x = (col + x) / grid_w # center position, unit: image width\n",
        "              y = (row + y) / grid_h # center position, unit: image height\n",
        "              w = anchors[b][0] * np.exp(w) / net_w # unit: image width\n",
        "              h = anchors[b][1] * np.exp(h) / net_h # unit: image height\n",
        "\n",
        "              box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n",
        "              #box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, None, classes)\n",
        "\n",
        "              boxes.append(box)\n",
        "\n",
        "      boxes_all += boxes\n",
        "\n",
        "    # Correct boxes\n",
        "    boxes_all = correct_yolo_boxes(boxes_all, image_h, image_w, net_h, net_w)\n",
        "\n",
        "    return boxes_all\n",
        "\n",
        "def correct_yolo_boxes(boxes_, image_h, image_w, net_h, net_w):\n",
        "    boxes = deepcopy(boxes_)\n",
        "    if (float(net_w)/image_w) < (float(net_h)/image_h):\n",
        "        new_w = net_w\n",
        "        new_h = (image_h*net_w)/image_w\n",
        "    else:\n",
        "        new_h = net_w\n",
        "        new_w = (image_w*net_h)/image_h\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        x_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n",
        "        y_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n",
        "\n",
        "        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
        "        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
        "        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
        "        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)\n",
        "    return boxes\n",
        "\n",
        "def do_nms(boxes_, nms_thresh, obj_thresh):\n",
        "    boxes = deepcopy(boxes_)\n",
        "    if len(boxes) > 0:\n",
        "        num_class = len(boxes[0].classes)\n",
        "    else:\n",
        "        return\n",
        "\n",
        "    for c in range(num_class):\n",
        "        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n",
        "\n",
        "        for i in range(len(sorted_indices)):\n",
        "            index_i = sorted_indices[i]\n",
        "\n",
        "            if boxes[index_i].classes[c] == 0: continue\n",
        "\n",
        "            for j in range(i+1, len(sorted_indices)):\n",
        "                index_j = sorted_indices[j]\n",
        "\n",
        "                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n",
        "                    boxes[index_j].classes[c] = 0\n",
        "\n",
        "    new_boxes = []\n",
        "    for box in boxes:\n",
        "        label = -1\n",
        "\n",
        "        for i in range(num_class):\n",
        "            if box.classes[i] > obj_thresh:\n",
        "                label = i\n",
        "                # print(\"{}: {}, ({}, {})\".format(labels[i], box.classes[i]*100, box.xmin, box.ymin))\n",
        "                box.label = label\n",
        "                box.score = box.classes[i]\n",
        "                new_boxes.append(box)\n",
        "\n",
        "    return new_boxes\n",
        "\n",
        "\n",
        "from PIL import ImageDraw, ImageFont\n",
        "import colorsys\n",
        "\n",
        "def draw_boxes(image_, boxes, labels):\n",
        "    image = image_.copy()\n",
        "    image_w, image_h = image.size\n",
        "    font = ImageFont.truetype(font='/usr/share/fonts/truetype/liberation/LiberationMono-Regular.ttf',\n",
        "                    size=np.floor(3e-2 * image_h + 0.5).astype('int32'))\n",
        "    thickness = (image_w + image_h) // 300\n",
        "\n",
        "    # Generate colors for drawing bounding boxes.\n",
        "    hsv_tuples = [(x / len(labels), 1., 1.)\n",
        "                  for x in range(len(labels))]\n",
        "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
        "    colors = list(\n",
        "        map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
        "    np.random.seed(10101)  # Fixed seed for consistent colors across runs.\n",
        "    np.random.shuffle(colors)  # Shuffle colors to decorrelate adjacent classes.\n",
        "    np.random.seed(None)  # Reset seed to default.\n",
        "\n",
        "    for i, box in reversed(list(enumerate(boxes))):\n",
        "        c = box.get_label()\n",
        "        predicted_class = labels[c]\n",
        "        score = box.get_score()\n",
        "        top, left, bottom, right = box.ymin, box.xmin, box.ymax, box.xmax\n",
        "\n",
        "        label = '{} {:.2f}'.format(predicted_class, score)\n",
        "        draw = ImageDraw.Draw(image)\n",
        "        label_size = draw.textsize(label, font)\n",
        "        #label_size = draw.textsize(label)\n",
        "\n",
        "        top = max(0, np.floor(top + 0.5).astype('int32'))\n",
        "        left = max(0, np.floor(left + 0.5).astype('int32'))\n",
        "        bottom = min(image_h, np.floor(bottom + 0.5).astype('int32'))\n",
        "        right = min(image_w, np.floor(right + 0.5).astype('int32'))\n",
        "        print(label, (left, top), (right, bottom))\n",
        "\n",
        "        if top - label_size[1] >= 0:\n",
        "            text_origin = np.array([left, top - label_size[1]])\n",
        "        else:\n",
        "            text_origin = np.array([left, top + 1])\n",
        "\n",
        "        # My kingdom for a good redistributable image drawing library.\n",
        "        for i in range(thickness):\n",
        "            draw.rectangle(\n",
        "                [left + i, top + i, right - i, bottom - i],\n",
        "                outline=colors[c])\n",
        "        draw.rectangle(\n",
        "            [tuple(text_origin), tuple(text_origin + label_size)],\n",
        "            fill=colors[c])\n",
        "        draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n",
        "        #draw.text(text_origin, label, fill=(0, 0, 0))\n",
        "        del draw\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wJjl_umy-DX"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8SzjAnb5ubi"
      },
      "source": [
        "# What is YOLO?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG9wqmq4Qd2_"
      },
      "source": [
        "\n",
        "The “You Only Look Once,” or YOLO, family of models are a series of end-to-end deep learning models designed for fast object detection, developed by Joseph Redmon, et al. and first proposed in the 2015 paper titled “[You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640).” The model has been updated since then. We'll focus on YOLOv3, which is described in this very interesting [technical report](https://pjreddie.com/media/files/papers/YOLOv3.pdf). We'll also walk through the basic idea of the algorithm. For more details about it, definitely check out the papers!\n",
        "\n",
        "The approach involves a single deep convolutional neural network (DarkNet which is based on the VGG model we used before) that splits the input into a grid of cells and each cell directly predicts a bounding box and object classification. The result is a large number of candidate bounding boxes that are consolidated into a final prediction by a post-processing step.\n",
        "\n",
        "For example, an image may be divided into a 7×7 grid and each cell in the grid may predict 2 bounding boxes, resulting in 98 proposed bounding box predictions. The class probabilities map and the bounding boxes with confidences are then combined into a final set of bounding boxes and class labels.\n",
        "\n",
        "In summary, to make object detection on one input image, the first step is a forward pass of the DarkNet; the second step is the post-processing on the DarkNet output to get the final bounding boxes prediction.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP3N7y0pPJb-"
      },
      "source": [
        "# How does YOLO work?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc-OEZJeyEms"
      },
      "source": [
        "Before we proceed to build the YOLO model, let's first define the **anchor boxes**, which are several pre-defined bounding boxes with useful shapes and sizes that are tailored based on the object shapes in the training dataset.\n",
        "\n",
        "There are 9 anchor boxes in total. As we'll talk about later, the detection is performed on 3 scales. Therefore, the anchor boxes are divided into 3 groups, each corresponding to 1 scale.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaEX1KApyaHV"
      },
      "source": [
        "anchors = [[[116,90], [156,198], [373,326]], [[30,61], [62,45], [59,119]], [[10,13], [16,30], [33,23]]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpK7kGhVycHY"
      },
      "source": [
        "\n",
        "The 9 anchor boxes are plotted below. They can cover a variety of shapes and sizes.\n",
        "\n",
        "<img src=\"http://www.programmersought.com/images/401/891354390c3aab3f1ab1fd0db3110bf9.png\" width=\"400\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v7dgrxMggcP"
      },
      "source": [
        "Now, let's load the image that we'll apply object detection on. To load the image, we'll use the `Image` module in the package `PIL`, which is commonly used  for image processing. The image is saved as a `PIL image` in the variable `image_pil`. We can get the width and the height of the image by accessing the `size` attribute of the image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kna4URqwgxR3"
      },
      "source": [
        "from PIL import Image\n",
        "from matplotlib import  pyplot as plt\n",
        "\n",
        "image_path = '/content/data/image.jpg'\n",
        "\n",
        "image_pil = Image.open(image_path)\n",
        "image_w, image_h = image_pil.size\n",
        "print(\"The type of the saved image is {}\".format(type(image_pil)))\n",
        "plt.imshow(image_pil)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PangGSekeos"
      },
      "source": [
        "### Image Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqI9ik44jiv3"
      },
      "source": [
        "The input size of DarkNet is `(416, 416)`, so we need to preprocess our image into the required size by resizing our image, keeping the aspect ratio consistent, and padding the left out areas with the grey color, which is `(128,128,128)` in RGB. We have implemented the preprocessing in the `preprocess_input(image, net_h, net_w)` function, which takes the orininal image, the target height and width `net_h, net_w` as input and returns the new image in the required size.\n",
        "\n",
        "In the chunk below, let's do the preprocessing and plot the new image to check the result!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H694I4RkWjH"
      },
      "source": [
        "net_h, net_w = 416, 416\n",
        "new_image = preprocess_input(image_pil, net_h, net_w)\n",
        "# Checking the new image\n",
        "plt.imshow(new_image[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taZ8Ux4bmIxL"
      },
      "source": [
        "### DarkNet Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUkj4NHMcEUj"
      },
      "source": [
        "The main part of the YOLO algorithm is the DarkNet model, which is basiclly a Convolutional Neural Network, with some special designs, like upsampling layers and detection layers.\n",
        "\n",
        "Here is how the architecture of DarkNet looks like:\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/2000/1*d4Eg17IVJ0L41e7CTWLLSg.png\" width=\"1000\"/>\n",
        "\n",
        "**The residual blocks** in the picture contain layers that are similar to the CNN models we built before, eg. convolutional layers `Conv2D`, max pooling layers `MaxPooling2D`, activation layers `Activation('relu')`. The network just stacks a lot more layers than the model we built before.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjJET2R__7tm"
      },
      "source": [
        "**How to make detections at 3 different scales?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WypkCU7Fm5Ev"
      },
      "source": [
        "Besides the stuff that we are familiar with, the most salient feature of YOLOv3 DarkNet is that it makes detections at three different scales, which are precisely given by downsampling the dimensions of the input image by 32, 16 and 8 respectively.\n",
        "\n",
        "The first detection is made by the 82nd layer. For the first 81 layers, the image is down sampled by the network, such that the 81st layer has a stride of 32. If we have an image of 416 x 416, the resultant feature map would be of size 13 x 13.\n",
        "\n",
        "The feature map size at layer 94 and 106 grows bigger because of the upsampling layers. The feature maps are upsampled by 2x to dimensions of 26 x 26 and 52 x 52 respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik1TiYtjAgv8"
      },
      "source": [
        "**What exactly are the DarkNet outputs?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alArj1hP_3SG"
      },
      "source": [
        "The eventual detection output is generated by applying detection kernels on feature maps at the three different places in the network.\n",
        "\n",
        "For each grid cell, we'll consider several possible bounding boxes that are centered at the given grid cell. Then for each considered bounding box, the model predicts t<sub>x</sub>, t<sub>y</sub>, t<sub>w</sub>, t<sub>h</sub>, an objectness score, and class scores.\n",
        "- t<sub>x</sub>, t<sub>y</sub>, t<sub>w</sub>, t<sub>h</sub> are related to predicting the exact position and shape of the considered bounding box.\n",
        "- The objectness score is the model's prediction about how likely the considered bounding box has a complete object inside it.\n",
        "- Class scores are the predicted probability over all the object classes.\n",
        "\n",
        "Therefore, the shape of the detection kernel is 1 x 1 x (B x (4 + 1 + C)). Here, 1 x 1 means the kernel only looks at one grid cell at one time. B is the number of bounding boxes a cell on the feature map can predict, \"4\" is for the 4 bounding box attributes (t<sub>x</sub>, t<sub>y</sub>, t<sub>w</sub>, t<sub>h</sub>) and \"1\" for the object confidence. C is the number of object classes.\n",
        "\n",
        "The model will consider bounding boxes based on the 3 anchor boxes defined before, so B = 3. As YOLO is trained on COCO (a large-scale object detection dataset), which contains 80 object catogories, C = 80. Therefore, the kernel size is 1 x 1 x 255. The feature map produced by this kernel has identical height and width of the previous feature map, and has detection attributes along the depth as described above.\n",
        "\n",
        "The following picture illustrates how this works.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1200/0*3A8U0Hm5IKmRa6hu.png\" width=\"500\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie0s7V0b0b_h"
      },
      "source": [
        "### Forward Pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET3JrlF-0izF"
      },
      "source": [
        "Now, let's load a fully trained DarkNet model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-ALek3J0Zk4"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load model\n",
        "darknet = tf.keras.models.load_model(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1InpAytA1j8i"
      },
      "source": [
        "Just as how we got the classification predictions from the perceptron, CNN and VGG models, let's call the `model.predict(input_data)` function to do a forward pass on our preprocessed image `new_image`! After getting the output, we check the structure of the output!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA8LJxlV2ne7"
      },
      "source": [
        "yolo_outputs = darknet.predict(new_image)\n",
        "print(len(yolo_outputs))\n",
        "print(yolo_outputs[0].shape)\n",
        "print(yolo_outputs[1].shape)\n",
        "print(yolo_outputs[2].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUjzOeRaPSaE"
      },
      "source": [
        "# Bounding Box Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-O9CWNg5pcS"
      },
      "source": [
        "We now have DarkNet's detection predictions for all the possible bounding boxes centered at each grid cell position, but to get the final detection results, which are the bounding boxes that the model is confident of, we need to apply a threshold to filter the results.\n",
        "\n",
        "Besides, there might be multiple bounding boxes that are detecting the same object. We need to remove the overlapping bounding boxes and only leave the best ones.\n",
        "\n",
        "Here are some post-presessing steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugs_ntcEOK8q"
      },
      "source": [
        "\n",
        "\n",
        "*   `decode_netout(yolo_outputs, obj_thresh, anchors, image_h, image_w, net_h, net_w)` takes the DarkNet output feature maps `yolo_outputs` as input, and returns all the predicted bounding boxes that have a higher objectness than the objectness threshold `obj_thresh`\n",
        "*   `do_nms(boxes, nms_thresh, obj_thresh)` means Non-Maximal Suppression, which a commonly used post-processing step for object detection. It  removes  all the bounding boxes that have a big (higher overlap than the `nms_thresh`) overlap with other better bounding boxes.\n",
        "*   `draw_boxes(image_pil, boxes, labels, obj_thresh)` draws the final bounding boxes on the input image and return the detection image as a `PIL image`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHvF90RH5fXX"
      },
      "source": [
        "### Post-processing for bounding box prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjxtyuXWSbvh"
      },
      "source": [
        "First, let's define the thresholds mentioned above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDQ-zqD1Sh-t"
      },
      "source": [
        "obj_thresh = 0.4\n",
        "nms_thresh = 0.45"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE9FP_-5R3Km"
      },
      "source": [
        "Let's make use of the functions above to get our final detection bounding boxes and plot the result!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ6OTsieDGUk"
      },
      "source": [
        "# Decode the output of the network\n",
        "boxes = decode_netout(yolo_outputs, obj_thresh, anchors, image_h, image_w, net_h, net_w)\n",
        "\n",
        "# Suppress non-maximal boxes\n",
        "boxes = do_nms(boxes, nms_thresh, obj_thresh)\n",
        "\n",
        "# Draw bounding boxes on the image using labels\n",
        "image_detect = draw_boxes(image_pil, boxes, labels)\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(image_detect)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC2EEvIwgvZT"
      },
      "source": [
        "### Non-Maximal Suppression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIMUP-LqhpGF"
      },
      "source": [
        "Let's explore what each post-processing step is doing! We can\n",
        "*   Check the number of boxes after each step\n",
        "*   Call the `draw_boxes(image_pil, boxes, labels, obj_thresh)` function to visualize the bounding boxes after each step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVzDICBQVWvb"
      },
      "source": [
        "# Decode\n",
        "boxes = decode_netout(yolo_outputs, obj_thresh, anchors, image_h, image_w, net_h, net_w)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(draw_boxes(image_pil, boxes, labels))\n",
        "plt.show()\n",
        "# NMS\n",
        "boxes = do_nms(boxes, nms_thresh, obj_thresh)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(draw_boxes(image_pil, boxes, labels))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqEuORB1jzJm"
      },
      "source": [
        "### Image Detection Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePIQrHdmkNlO"
      },
      "source": [
        "Our final goal is to detect objects in a video, which contains multiple frames (images). For better reusability and modularity, let's wrap all the code we wrote before in a function called `detect_image`, which takes the raw `PIL image` (without preprocessing) and other parameters as input, and returns the `PIl image` with detected bounding boxes and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q096a_cwlvUM"
      },
      "source": [
        "def detect_image(image_pil, obj_thresh = 0.4, nms_thresh = 0.45, darknet=darknet, net_h=416, net_w=416, anchors=anchors, labels=labels):\n",
        "\n",
        "  # Preprocessing\n",
        "  image_w, image_h = image_pil.size\n",
        "  new_image = preprocess_input(image_pil, net_h, net_w)\n",
        "\n",
        "  # DarkNet\n",
        "  yolo_outputs = darknet.predict(new_image)\n",
        "\n",
        "  # Decode the output of the network\n",
        "  boxes = decode_netout(yolo_outputs, obj_thresh, anchors, image_h, image_w, net_h, net_w)\n",
        "\n",
        "  # Suppress non-maximal boxes\n",
        "  boxes = do_nms(boxes, nms_thresh, obj_thresh)\n",
        "\n",
        "  # Draw bounding boxes on the image using labels\n",
        "  image_detect = draw_boxes(image_pil, boxes, labels)\n",
        "\n",
        "  return image_detect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK2Pm3pRmPCu"
      },
      "source": [
        "#@title Checking whether our function works { display-mode: \"form\" }\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(detect_image(image_pil))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPn2dkmWn7Fk"
      },
      "source": [
        "### Thresholds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlU20D9aoBSs"
      },
      "source": [
        "Up till now, we used default values for the 2 thresholds, `objectness threshold` and `nms_threshold`. Let's make use of the `detect_image`function defined above and try different values for the 2 thresholds in the range of 0-1 and see the changes in the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Dt0UR4xpPG-"
      },
      "source": [
        "# Lower objectness threshold -> more object predictions are accepted (more predictions)\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(detect_image(image_pil, obj_thresh = 0.2, nms_thresh = 0.45))\n",
        "plt.show()\n",
        "\n",
        "# Higher nms threshold -> Allowing more overlapping bounding boxes (more predictions)\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(detect_image(image_pil, obj_thresh = 0.4, nms_thresh = 0.8))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q041DhjwP7wh"
      },
      "source": [
        "# Detection on Videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsNm7ivW0fwl"
      },
      "source": [
        "A video is just a sequence of frames (images). Therefore, once we can use YOLO to detect objects on images, it's easy to extend this to videos. To deal with videos, we'll use the OpenCV package, which is called `cv2` in Python. To know more, here is a [tutorial](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html).\n",
        "\n",
        "The code below will open one video, create a new video file, read the input video frame-by-frame, and write each frame into the new video.\n",
        "\n",
        "We will modify the code to get the object detection result on the input video!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHsORdfL3Weq"
      },
      "source": [
        "Note that the image input for the `detect_image` function is a `PIL image`, but here we are loading the input video using `OpenCV`. These 2 image formats are different, so we need to convert `OpenCV` to `PIL` for detection, and convert back to write the frame into the new video.\n",
        "\n",
        "The conversion can be done as follows\n",
        "```\n",
        "# OpenCV -> PIL\n",
        "image_pil = Image.fromarray(cv2.cvtColor(image_cv2, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# PIL -> OpenCV\n",
        "image_cv2 = cv2.cvtColor(np.asarray(image_pil), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLsykmGe2EOP"
      },
      "source": [
        "import cv2\n",
        "\n",
        "def detect_video(video_path, output_path, obj_thresh = 0.4, nms_thresh = 0.45, darknet=darknet, net_h=416, net_w=416, anchors=anchors, labels=labels):\n",
        "    vid = cv2.VideoCapture(video_path)\n",
        "    if not vid.isOpened():\n",
        "        raise IOError(\"Couldn't open webcam or video\")\n",
        "    video_FourCC    = int(vid.get(cv2.CAP_PROP_FOURCC))\n",
        "    video_FourCC = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    video_fps       = vid.get(cv2.CAP_PROP_FPS)\n",
        "    video_size      = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "                        int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "\n",
        "    out = cv2.VideoWriter(output_path, video_FourCC, video_fps, video_size)\n",
        "\n",
        "    num_frame = 0\n",
        "    while vid.isOpened():\n",
        "      ret, frame = vid.read()\n",
        "      num_frame += 1\n",
        "      print(\"=== Frame {} ===\".format(num_frame))\n",
        "      if ret:\n",
        "          frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "          image = Image.fromarray(frame)\n",
        "\n",
        "          result = detect_image(image)\n",
        "\n",
        "          new_frame = np.asarray(result)\n",
        "          new_frame = cv2.cvtColor(new_frame, cv2.COLOR_RGB2BGR)\n",
        "          out.write(new_frame)\n",
        "      else:\n",
        "          break\n",
        "    vid.release()\n",
        "    out.release()\n",
        "    print(\"New video saved!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEHFzqQn5R5h"
      },
      "source": [
        "Videos to test out the AI tool on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-fRC2TW8Qkn"
      },
      "source": [
        "video_path = '/content/data/video1.mp4'\n",
        "output_path = '/content/data/video1_detected.mp4'\n",
        "detect_video(video_path, output_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}